{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jimi Togni - RA: 226359"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1 – Classificação binária\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando pacotes essenciais\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from scipy.spatial.distance import correlation as dcorrelation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_confusion_matrix(y, yh):\n",
    "    cls = np.unique(y)\n",
    "    confusion_m = np.zeros((cls.size, cls.size))\n",
    "    for i, yhto in enumerate(y):\n",
    "        for cl in cls:\n",
    "            if yhto == yh[i] and yhto == cl:\n",
    "                confusion_m[cl, cl] += 1\n",
    "                break\n",
    "            elif yhto != yh[i]:\n",
    "                confusion_m[yhto, yh[i]] += 1\n",
    "                break\n",
    "    return confusion_m\n",
    "\n",
    "def gen_classification_report(confusion_m):\n",
    "    cls, _ = confusion_m.shape\n",
    "    cls = range(cls)\n",
    "    report = {(i+1): {\"precision\": 0, \"recall\": 0, \"f1-score\": 0, \"support\": 0} for i in cls}\n",
    "    for r in cls:\n",
    "        for c in cls:\n",
    "            if r == c:\n",
    "                tp = confusion_m[r, c]\n",
    "                tn = np.diagonal(confusion_m) - tp\n",
    "                fp = np.sum(confusion_m[:, c])\n",
    "                fn = np.sum(confusion_m[r, :])\n",
    "                P, R = np.round(tp / fp, 2), np.round(tp / fn, 2)\n",
    "                report[r+1][\"support\"] = tp + (fn - tp)\n",
    "                report[r+1][\"precision\"] = P\n",
    "                report[r+1][\"recall\"] = R\n",
    "                report[r+1][\"f1-score\"] = np.round((2 * (P * R)) / (P + R), 2)\n",
    "    return report\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    return pd.get_dummies(y).values\n",
    "\n",
    "class LogisticRegressionImpl():\n",
    "        \n",
    "    def __init__(self, fit_intercept=True, decay=1e-4, batch_size=32, weights=None):\n",
    "        self.w = weights\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self._decay = decay\n",
    "        self._batch_size = batch_size\n",
    "    \n",
    "    def _one_hot_encode(self, y):\n",
    "        return pd.get_dummies(y).values\n",
    "        \n",
    "    def __softmax(self, z):\n",
    "        e_x = np.exp(z.T - np.max(z, axis=1)) \n",
    "        return (e_x / e_x.sum(axis=0)).T\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1./(1 + np.exp(-z))\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def _shuffle(self, X, y):\n",
    "        permutation = np.random.permutation(X.shape[0])\n",
    "        if self.__cls > 2:\n",
    "            return X[permutation, :], y[permutation, :]\n",
    "        return X[permutation, :], y[permutation]\n",
    "    \n",
    "    def train(self, x, y, lr=1e-4, epochs=10000):\n",
    "        # Check to see the number of classes \n",
    "        uniq = np.unique(y)\n",
    "        self.__activation = self.__sigmoid\n",
    "        self.__cls = uniq.shape[0]\n",
    "        if self.__cls > 2:\n",
    "            y = self._one_hot_encode(y)\n",
    "            self.__activation = self.__softmax\n",
    "        # Train and validation sets\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=0.7)\n",
    "        # Should we add a intercept\n",
    "        x_train, x_val = self._add_intercept(x_train), self._add_intercept(x_val)\n",
    "        # Number of samples\n",
    "        self.N = y.size\n",
    "        # Initialize weights\n",
    "        if self.w is None:\n",
    "            self.w = np.random.rand(x_train.shape[1], ) * np.sqrt(2 / (self.N + 2))\n",
    "            if self.__cls > 2:\n",
    "                self.w = np.random.rand(x_train.shape[1], self.__cls) * np.sqrt(2 / (self.N + self.__cls))\n",
    "\n",
    "        # Train loop Gradient Descent\n",
    "        J, ACC = [], []\n",
    "        mb = np.ceil(x_train.shape[0] / self._batch_size).astype(np.int32)\n",
    "        for i in range(epochs):\n",
    "            # Shuffle dataset in each epoch\n",
    "            x_train, y_train = self._shuffle(x_train.copy(), y_train.copy())\n",
    "            r = 0\n",
    "            for _ in range(mb):\n",
    "                # Mini batch crop\n",
    "                ini, end = r * self._batch_size, (r + 1) * self._batch_size\n",
    "                batch_X, batch_y = x_train[ini:end, :], y_train[ini:end]\n",
    "                if self.__cls > 2:\n",
    "                    batch_X, batch_y = x_train[ini:end, :], y_train[ini:end, :]\n",
    "                r += 1\n",
    "                # Forward\n",
    "                yh = self.__activation(np.dot(batch_X, self.w))\n",
    "                # Gradient\n",
    "                dw = (1 / self.N) * np.dot(batch_X.T, (yh - batch_y))\n",
    "                # Weight adjust with regularization\n",
    "                self.w = (self.w - (lr * dw)) - (((lr * self._decay) / self.N) * self.w)\n",
    "            # Validation Loss\n",
    "            h = self.__activation(np.dot(x_val, self.w))\n",
    "            # Accuracy\n",
    "            ACC.append(np.round(self.__acc(y_val, h), 5))\n",
    "            # Loss\n",
    "            J.append(np.round(self.__loss(y_val, h), 5))\n",
    "        return J, ACC\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        x = self._add_intercept(x)\n",
    "        return self.__activation(np.dot(x, self.w))\n",
    "    \n",
    "    def predict(self, x, threshold=0.5):\n",
    "        if self.__cls > 2:\n",
    "            return np.argmax(self.predict_proba(x), axis=1)\n",
    "        return (self.predict_proba(x) >= threshold).astype(np.int)\n",
    "    \n",
    "    def __acc(self, y, yh):\n",
    "        Y, YH = y, (yh >= .5).astype(np.int)\n",
    "        if self.__cls > 2:\n",
    "            Y, YH = np.argmax(y, axis=1), np.argmax(yh, axis=1)\n",
    "        return (\n",
    "                np.round(\n",
    "                    np.mean([y == yh for y, yh in zip(Y, YH)]), 2\n",
    "                )\n",
    "                * 100\n",
    "            )\n",
    "    \n",
    "    def __loss(self, y, yh):\n",
    "        L2 = (self._decay / (2 * self.N)) * np.sum([np.sum(np.square(w)) for w in self.w])\n",
    "        if self.__cls > 2:\n",
    "            n_samples = y.shape[0]\n",
    "            logp = -np.log(yh[np.arange(n_samples), y.argmax(axis=1)])\n",
    "            loss = np.sum(logp) / n_samples\n",
    "            return loss + L2\n",
    "        bce = -np.sum(np.multiply(y, np.log(yh)) + np.multiply((1-y), np.log(1 - yh)))\n",
    "        return ((1 / self.N) * bce) + L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regressao Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regressao Logistica\n",
    "class LogisticRegression():\n",
    "    \n",
    "    def __init__(self, fit_intercept=True, reg=1e-20, weights=None):\n",
    "        self.w = weights #pesos\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self._reg = reg #regularização\n",
    "        \n",
    "    def __sigmoid(self, z):\n",
    "        return 1.(1 + np.exp(-z)) #função sigmoid\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def train(self, x, y, lr=1e-5, epochs=10000):\n",
    "        x =self._add_intercept(x)\n",
    "        self.N = y.size\n",
    "        \n",
    "        if self.w is None:\n",
    "            self.w = np.random.rand(x.shape[1], )* np.sqrt(2/(self.N + 2))\n",
    "        \n",
    "        J = []\n",
    "        \n",
    "        for i in range (epochs):\n",
    "            #forward\n",
    "            yh = self.__sigmoid(np.dot(x, self.w))\n",
    "            \n",
    "            #gradiente\n",
    "            dw = (1 / self.N) * np.dot(x.T, (yh -y))\n",
    "            \n",
    "            #ajuste de pesos\n",
    "            self.w = (self.w - (lr * dw)) - (((lr * self._reg / self.N) * self.w))\n",
    "            \n",
    "            #perdas\n",
    "            h = self.__sigmoid(np.dot(x_val, self.w))\n",
    "            loss = self.__loss(y_val, h)\n",
    "            J.append(np.round(loss, 5))\n",
    "        \n",
    "        return self, J\n",
    "    \n",
    "    def predict_prob(self, x):\n",
    "        x = self._add_intercept(x)\n",
    "        return self.__sigmoid(np.dot(x, self.we))\n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problema: identificação do gênero do locutor a partir de trechos de voz\n",
    "Base de dados: dados_voz_genero.csv\n",
    "https://www.mldata.io/dataset-details/gender_voice/\n",
    "\n",
    "Você dispõe de um conjunto de dados contendo 3168 amostras rotuladas. Cada\n",
    "amostra é descrita por 19 atributos acústicos extraídos de trechos gravados de voz,\n",
    "considerando a faixa de frequências de 0 a 280 Hz. A última coluna corresponde ao\n",
    "rótulo associado a cada padrão, sendo igual a ‘1’ para o gênero masculino, e ‘0’ para o\n",
    "gênero feminino.\n",
    "\n",
    "a) Faça uma análise das características dos atributos de entrada considerando os\n",
    "respectivos histogramas e as medidas de correlação entre eles.\n",
    "\n",
    "b) Construa, então, o modelo de regressão logística para realizar a classificação\n",
    "dos padrões. Para isso, reserve uma parte dos dados (e.g., 20%) para teste,\n",
    "usando todas as demais amostras para o treinamento do modelo. Pensem na\n",
    "pertinência e na possibilidade de realizar algum pré-processamento nos dados\n",
    "(e.g., normalização).\n",
    "Apresente e discuta os seguintes resultados com relação ao conjunto de teste:\n",
    "\n",
    " A curva ROC;\n",
    "\n",
    " A curva de evolução da -medida em função do valor do threshold de\n",
    "decisão.\n",
    "\n",
    "c) Indique qual seria o valor mais adequado para o threshold de decisão e por\n",
    "quê. Empregando, então, esse threshold, obtenha a matriz de confusão e a\n",
    "acurácia do classificador para o conjunto de teste. Comente os resultados\n",
    "obtidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data(3168, 21)\n",
      "First 10 rows   Unnamed: 0        sd    median       Q25       Q75       IQR       skew  \\\n",
      "0           0  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
      "1           1  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
      "2           2  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
      "3           3  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
      "4           4  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
      "\n",
      "          kurt    sp.ent       sfm  ...  centroid   meanfun    minfun  \\\n",
      "0   274.402906  0.893369  0.491918  ...  0.059781  0.084279  0.015702   \n",
      "1   634.613855  0.892193  0.513724  ...  0.066009  0.107937  0.015826   \n",
      "2  1024.927705  0.846389  0.478905  ...  0.077316  0.098706  0.015656   \n",
      "3     4.177296  0.963322  0.727232  ...  0.151228  0.088965  0.017798   \n",
      "4     4.333713  0.971955  0.783568  ...  0.135120  0.106398  0.016931   \n",
      "\n",
      "     maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
      "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000    1.0  \n",
      "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632    1.0  \n",
      "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512    1.0  \n",
      "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119    1.0  \n",
      "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274    1.0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "#Visualizando os dados\n",
    "\n",
    "df = pd.read_csv('dados_voz_genero.csv')\n",
    "print(f\"Shape of data{df.shape}\")\n",
    "print(f\"First 10 rows{df.head(5)}\")\n",
    "\n",
    "\n",
    "#convertendo para numpy\n",
    "#df = np.array(data)\n",
    "#print(f\"Shape of np_data{df.shape}\")\n",
    "\n",
    "#np_data[:10, [1,3]]\n",
    "#primeiras 10 linhas e a primeira e ultima colunas \n",
    "#data_np[:10, [0,-1]]\n",
    "#data.head(10)\n",
    "\n",
    "#colunas\n",
    "#data_np.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### heatmap ta bugado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ed9373cf6012>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'skew'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kurt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sp.ent'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'median'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dfrange'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'modindx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'maxdom'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Unpivot the dataframe, so we can get pair of arrays for x and y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mcorr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#matrix of correlation\n",
    "\n",
    "def heatmap (x, y, size):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    #mapping from column names to integer coordinates\n",
    "    x_labels = [v for v in sorted(x.unique())]\n",
    "    y_labels = [v for v in sorted(y.unique())]\n",
    "    x_to_num = {p[1]:p[0] for p in enumerate(x_labels)} \n",
    "    y_to_num = {p[1]:p[0] for p in enumerate(y_labels)} \n",
    "    \n",
    "    size_scale = 500\n",
    "    \n",
    "    ax.scatter(\n",
    "        x = x.map(x_to_num), #map for x\n",
    "        y = y.map(y_to_num), #map for x\n",
    "        s = size * size_scale,\n",
    "        marker = 's'\n",
    "    )\n",
    "    \n",
    "    # show column axes labels\n",
    "    ax.set_xticks([x_to_num[v] for v in x_labels])\n",
    "    ax.set_xticklabels(x_labels, rotation=90, horizontalalignment='right')\n",
    "    ax.set_yticks([y_to_num[v] for v in y_labels])\n",
    "    ax.set_yticklabels(y_labels)\n",
    "    \n",
    "\n",
    "columns = ['skew', 'kurt', 'sp.ent', 'median', 'dfrange', 'modindx', 'maxdom']\n",
    "corr = data[columns].corr()\n",
    "corr = pd.melt(corr.reset_index(), id_vars='index') # Unpivot the dataframe, so we can get pair of arrays for x and y\n",
    "corr.columns = ['x', 'y', 'value']\n",
    "\n",
    "heatmap(x=corr['x'], y=['y'], size=corr['value'].abs())\n",
    "\n",
    "#ax = sns.heatmap(corr, vmin=-1, vmax=1, center=0, cmap=sns.diverging_palette(20, 220, n=200), square=True)\n",
    "#ax.set_xticklabels(ax.get_xticklabels(), rotation=90, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selecionando os dados que possuem maior relação com audio de acordo com a matriz de correlaçao**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapa de correl total\n",
    "#columns = ['skew', 'kurt', 'sp.ent', 'median', 'dfrange', 'modindx', 'maxdom']\n",
    "corr = data.corr()\n",
    "\n",
    "ax = sns.heatmap(corr, vmin=-1, vmax=1, center=0, \n",
    "                 cmap=sns.diverging_palette(20, 220, n=200),\n",
    "                 square=True)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), \n",
    "                  rotation=90, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['skew', 'kurt', 'sp.ent',\n",
    "           'median', 'dfrange', 'modindx', 'maxdom']\n",
    "corr = data[columns].corr()\n",
    "\n",
    "ax = sns.heatmap(corr, vmin=-1, vmax=1, center=0, \n",
    "                 cmap=sns.diverging_palette(20, 220, n=200),\n",
    "                 square=True)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), \n",
    "                  rotation=90, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['skew', 'kurt', 'centroid',\n",
    "           'mode', 'sfm', 'meanfun', 'Q75', 'Q25']\n",
    "corr = data[columns].corr()\n",
    "\n",
    "ax = sns.heatmap(corr, vmin=-1, vmax=1, center=0, \n",
    "                 cmap=sns.diverging_palette(20, 220, n=200),\n",
    "                 square=True)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), \n",
    "                  rotation=90, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['skew', 'kurt', 'sp.ent',\n",
    "           'mode', 'sfm', 'centroid', 'label']\n",
    "corr = data[columns].corr()\n",
    "\n",
    "ax = sns.heatmap(corr, vmin=-1, vmax=1, center=0, \n",
    "                 cmap=sns.diverging_palette(20, 220, n=200),\n",
    "                 square=True)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), \n",
    "                  rotation=90, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df[['skew', 'kurt', 'sp.ent',\n",
    "           'mode', 'sfm', 'centroid', 'label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Histograma com os melhores parametros relacionados de acordo com a matriz de correlação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title(\"skew\")\n",
    "plt.hist(columns[\"skew\"], bins='auto', color='C1')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.title(\"kurt\")\n",
    "plt.hist(columns[\"kurt\"], bins='auto', color='C1')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.title(\"sp.ent\")\n",
    "plt.hist(columns[\"sp.ent\"], bins='auto', color='C1')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.title(\"mode\")\n",
    "plt.hist(columns[\"mode\"], bins='auto')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.title(\"sfm\")\n",
    "plt.hist(columns[\"sfm\"], bins='auto')\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.title(\"centroid\")\n",
    "plt.hist(columns[\"centroid\"], bins='auto')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#media, min, max, var, std, dos atributos selecionados\n",
    "\n",
    "#df.head(10)\n",
    "df_sel = []\n",
    "\n",
    "for field in ['skew', 'kurt', 'sp.ent', 'mode', 'sfm', 'centroid', 'label']:\n",
    "    df_sel.append([\n",
    "        field,\n",
    "        np.round(np.min(columns[field]), 5),\n",
    "        np.round(np.max(columns[field]), 5),\n",
    "        np.round(np.mean(columns[field]), 5),\n",
    "        np.round(np.median(columns[field]), 5),\n",
    "        np.round(np.var(columns[field]), 5),\n",
    "        np.round(np.std(columns[field]), 5)\n",
    "    ])\n",
    "    \n",
    "df_sef2 = pd.DataFrame(df_sel, columns=[\"Amostras\",\n",
    "                                        \"Min\",\n",
    "                                        \"Max\",\n",
    "                                        \"Media\",\n",
    "                                        \"Mediana\",\n",
    "                                        \"Variacao\",\n",
    "                                        \"Std\"\n",
    "                                       ])\n",
    "\n",
    "print(df_sef2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(sound_c[x_fields])\n",
    "nsound_c = scaler.transform(sound_c[x_fields])\n",
    "sound_c = pd.DataFrame(np.c_[nsound_c, sound_c[\"label\"].to_numpy()], columns=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for field in x_fields:\n",
    "    unique_elements, counts_elements = np.unique(sound_c[field], return_counts=True)\n",
    "    data.append([\n",
    "        field,\n",
    "        len(sound_c[field]),\n",
    "        np.round(np.min(sound_c[field]), 5),\n",
    "        np.round(np.max(sound_c[field]), 5),\n",
    "        np.round(np.mean(sound_c[field]), 5),\n",
    "        np.round(np.median(sound_c[field]), 5),\n",
    "        np.round(np.var(sound_c[field]), 5),\n",
    "        np.round(np.std(sound_c[field]), 5),\n",
    "        len(counts_elements)\n",
    "    ])\n",
    "df2 = pd.DataFrame(data, columns=[\"Field\", \"Qtd\", \"Min\", \"Max\", \"Mean\", \"Median\", \"Var.\", \"Std\", \"Unique\"])\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for field in x_fields:\n",
    "    unique_elements, counts_elements = np.unique(sound_c[field], return_counts=True)\n",
    "    data.append([\n",
    "        field,\n",
    "        len(sound_c[field]),\n",
    "        np.round(np.min(sound_c[field]), 5),\n",
    "        np.round(np.max(sound_c[field]), 5),\n",
    "        np.round(np.mean(sound_c[field]), 5),\n",
    "        np.round(np.median(sound_c[field]), 5),\n",
    "        np.round(np.var(sound_c[field]), 5),\n",
    "        np.round(np.std(sound_c[field]), 5),\n",
    "        len(counts_elements)\n",
    "    ])\n",
    "df2 = pd.DataFrame(data, columns=[\"Field\", \"Qtd\", \"Min\", \"Max\", \"Mean\", \"Median\", \"Var.\", \"Std\", \"Unique\"])\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for field in x_fields:\n",
    "    unique_elements, counts_elements = np.unique(sound_c[field], return_counts=True)\n",
    "    data.append([\n",
    "        field,\n",
    "        len(sound_c[field]),\n",
    "        np.round(np.min(sound_c[field]), 5),\n",
    "        np.round(np.max(sound_c[field]), 5),\n",
    "        np.round(np.mean(sound_c[field]), 5),\n",
    "        np.round(np.median(sound_c[field]), 5),\n",
    "        np.round(np.var(sound_c[field]), 5),\n",
    "        np.round(np.std(sound_c[field]), 5),\n",
    "        len(counts_elements)\n",
    "    ])\n",
    "df2 = pd.DataFrame(data, columns=[\"Field\", \"Qtd\", \"Min\", \"Max\", \"Mean\", \"Median\", \"Var.\", \"Std\", \"Unique\"])\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for field in x_fields:\n",
    "    unique_elements, counts_elements = np.unique(sound_c[field], return_counts=True)\n",
    "    data.append([\n",
    "        field,\n",
    "        len(sound_c[field]),\n",
    "        np.round(np.min(sound_c[field]), 5),\n",
    "        np.round(np.max(sound_c[field]), 5),\n",
    "        np.round(np.mean(sound_c[field]), 5),\n",
    "        np.round(np.median(sound_c[field]), 5),\n",
    "        np.round(np.var(sound_c[field]), 5),\n",
    "        np.round(np.std(sound_c[field]), 5),\n",
    "        len(counts_elements)\n",
    "    ])\n",
    "df2 = pd.DataFrame(data, columns=[\"Field\", \"Qtd\", \"Min\", \"Max\", \"Mean\", \"Median\", \"Var.\", \"Std\", \"Unique\"])\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = cols[:len(cols)-1].to_numpy()\n",
    "x = np.copy(df[fields].to_numpy())\n",
    "y = np.copy(df[\"label\"].to_numpy())\n",
    "# train test split 20%\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic function\n",
    "clf = LogisticRegressionImpl()\n",
    "J, ACC = clf.train(x_train, y_train, lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = clf.predict_proba(x_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Curva ROC\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"FP\")\n",
    "plt.ylabel(\"TP\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_resultados(y_test, y_pred):\n",
    "    with np.errstate(all='ignore'):\n",
    "        cfm = confusion_matrix(y_test, y_pred)\n",
    "        acertos = cfm[0, 0] + cfm[1, 1]\n",
    "        erros = cfm[0, 1] + cfm[1, 0]\n",
    "        precisao = np.round(cfm[0, 0] / (cfm[0, 0] + cfm[0, 1]), 2)\n",
    "        recall = np.round(cfm[0, 0] / (cfm[0, 0] + cfm[1, 0]), 2)\n",
    "        fscore = np.round((2 * recall * precisao) / (recall + precisao), 2)\n",
    "        acuracia = np.round(acertos / len(y_test) * 100, 2)\n",
    "        erros = np.round(erros / len(y_test) * 100, 2)\n",
    "    return acuracia, erros, recall, precisao, fscore\n",
    "    \n",
    "def print_resultados(acuracia, erros, recall, precisao, fscore):\n",
    "    print(\"Acurácia:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Acertos: {acuracia}%\")\n",
    "    print(f\"Erros  : {erros}%\")\n",
    "    print()\n",
    "    print(\"Recall | Precisão | F-Score\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"{recall}   | {precisao}     | {fscore}\")\n",
    "\n",
    "\n",
    "resultados = []\n",
    "for thre in np.linspace(0, 1, 41):\n",
    "    thre = np.round(thre, 2)\n",
    "    y_pred = clf.predict(x_test, threshold=thre)\n",
    "    resultados.append(np.concatenate((np.array([thre]), calculate_resultados(y_test, y_pred))))\n",
    "resultados = np.nan_to_num(np.array(resultados))\n",
    "\n",
    "df = pd.DataFrame(resultados, columns=[\"Threshold\", \"Acurácia\", \"Erros\", \"Recall\", \"Precisão\", \"F-Score\"])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Threshold x F-Score\")\n",
    "plt.plot(df[\"Threshold\"], df[\"F-Score\"], c=\"C3\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"F-Score\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Threshold x Acurácia\")\n",
    "plt.plot(df[\"Threshold\"], df[\"Acurácia\"], c=\"C0\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Acurácia\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ordered = df.sort_values(by=[\"F-Score\", \"Acurácia\"], ascending=False)\n",
    "\n",
    "print(\"Resultados:\")\n",
    "print(\"-\" * 30)\n",
    "print(ordered.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = ordered[\"Threshold\"].to_numpy()[0]\n",
    "print(f\"Melhor threshold encontrado usando os experimentos acima: {best_threshold}\")\n",
    "print()\n",
    "\n",
    "y_pred = clf.predict(x_test, threshold=best_threshold)\n",
    "\n",
    "print(\"Acurácia:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Acertos: {np.round(accuracy_score(y_test, y_pred) * 100, 2)}%\")\n",
    "print(f\"Erros  : {np.round((1-accuracy_score(y_test, y_pred)) * 100, 2)}%\")\n",
    "\n",
    "print()\n",
    "print(\"Relatório de Classificação:\")\n",
    "print(\"-\" * 30)\n",
    "report = classification_report(y_test.astype(np.int), y_pred.astype(np.int), output_dict=True)\n",
    "items = []\n",
    "for key, value in report.items():\n",
    "    if isinstance(value, dict) and key.isdigit():\n",
    "        items.append([\n",
    "            key,\n",
    "            np.round(value[\"precision\"], 2),\n",
    "            np.round(value[\"recall\"], 2),\n",
    "            np.round(value[\"f1-score\"], 2),\n",
    "            np.round(value[\"support\"], 2)\n",
    "        ])\n",
    "dfclass = pd.DataFrame(items, columns=[\"Classe\", \"Precisão\", \"Recall\", \"F1-Score\", \"Support\"])\n",
    "print(dfclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(y_test.astype(np.int), y_pred.astype(np.int))\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title(\"Loss x Epochs\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"J ()\")\n",
    "plt.plot(J, color=\"C4\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title(\"Accuracy x Epochs\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"J ()\")\n",
    "plt.plot(ACC, color=\"C3\")\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "im = plt.imshow(cfm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set(xticks=np.arange(cfm.shape[1]),\n",
    "       yticks=np.arange(cfm.shape[0]),\n",
    "       # ... and label them with the respective list entries\n",
    "       xticklabels=[\"female\", \"male\"], yticklabels=[\"female\", \"male\"],\n",
    "       title=\"Matriz de Confusão\",\n",
    "       ylabel='True label',\n",
    "       xlabel='Predicted label')\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "# Loop over data dimensions and create text annotations.\n",
    "thresh = cfm.max() / 2.\n",
    "for i in range(cfm.shape[0]):\n",
    "    for j in range(cfm.shape[1]):\n",
    "        ax.text(j, i, format(cfm[i, j], '.2f'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cfm[i, j] > thresh else \"black\")\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(1.5, -0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EX 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename, ground_truth=False):\n",
    "    columns = []\n",
    "    with open(filename, \"r\") as fh:\n",
    "        for line in fh.readlines():\n",
    "            columns.append([float(col) for col in line.strip().split()])\n",
    "    if ground_truth:\n",
    "        columns = np.array(columns).flatten().astype(np.int)\n",
    "    return pd.DataFrame(columns)\n",
    "\n",
    "har_x_train = read_dataset(\"./har_smartphone/X_train.txt\")\n",
    "har_x_test = read_dataset(\"./har_smartphone/X_test.txt\")\n",
    "har_y_train = read_dataset(\"./har_smartphone/y_train.txt\", ground_truth=True)\n",
    "har_y_test = read_dataset(\"./har_smartphone/y_test.txt\", ground_truth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Quantidade de amostras para treinamento : {har_x_train.shape[0]}\")\n",
    "print(f\"Quantidade de amostras para teste       : {har_x_test.shape[0]}\")\n",
    "print(f\"Quantidade total de colunas (features)  : {har_x_train.shape[1]}\")\n",
    "\n",
    "print()\n",
    "print(\"Apresentação dos primeiros 2 registros e algumas colunas:\")\n",
    "print(\"-\" * 30)\n",
    "print(har_x_train.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_har_train = har_x_train.to_numpy()\n",
    "x_har_test = har_x_test.to_numpy()\n",
    "y_har_train = har_y_train.to_numpy().flatten()\n",
    "y_har_test = har_y_test.to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic function\n",
    "clfsft = LogisticRegressionImpl()\n",
    "J, ACC = clfsft.train(x_har_train, y_har_train, lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_har_pred = clfsft.predict(x_har_test)\n",
    "y_har_test_on = np.argmax(one_hot_encode(y_har_test), axis=1)\n",
    "\n",
    "print(\"Aplicando o classificador no conjunto de teste\")\n",
    "print(\"-\" * 30)\n",
    "print()\n",
    "\n",
    "print(\"Acurácia:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Acertos: {np.round(accuracy_score(y_har_test_on, y_har_pred) * 100, 2)}%\")\n",
    "print(f\"Erros  : {np.round((1-accuracy_score(y_har_test_on, y_har_pred)) * 100, 2)}%\")\n",
    "\n",
    "print()\n",
    "print(\"Relatório de Classificação:\")\n",
    "print(\"-\" * 30)\n",
    "report = classification_report(y_har_test_on + 1, y_har_pred + 1, output_dict=True)\n",
    "items = []\n",
    "for key, value in report.items():\n",
    "    if isinstance(value, dict) and key.isdigit():\n",
    "        items.append([\n",
    "            key,\n",
    "            np.round(value[\"precision\"], 2),\n",
    "            np.round(value[\"recall\"], 2),\n",
    "            np.round(value[\"f1-score\"], 2),\n",
    "            np.round(value[\"support\"], 2)\n",
    "        ])\n",
    "dfclass = pd.DataFrame(items, columns=[\"Classe\", \"Precisão\", \"Recall\", \"F1-Score\", \"Support\"])\n",
    "print(dfclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"caminhada\", \"subindo escadas\", \"descendo escadas\", \"sentado\", \"em pé\", \"deitado\"]\n",
    "cfm = confusion_matrix(y_har_test_on, y_har_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title(\"Loss x Epoch\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"J ()\")\n",
    "plt.plot(J, color=\"C4\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title(\"Accuracy x Epoch\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"J ()\")\n",
    "plt.plot(ACC, color=\"C3\")\n",
    "\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "im = plt.imshow(cfm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set(xticks=np.arange(cfm.shape[1]),\n",
    "       yticks=np.arange(cfm.shape[0]),\n",
    "       # ... and label them with the respective list entries\n",
    "       xticklabels=labels, yticklabels=labels,\n",
    "       title=\"Matriz de Confusão\",\n",
    "       ylabel='True label',\n",
    "       xlabel='Predicted label')\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "# Loop over data dimensions and create text annotations.\n",
    "thresh = cfm.max() / 2.\n",
    "for i in range(cfm.shape[0]):\n",
    "    for j in range(cfm.shape[1]):\n",
    "        ax.text(j, i, format(cfm[i, j], '.2f'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cfm[i, j] > thresh else \"black\")\n",
    "ax.set_xlim(-0.5, 5.5)\n",
    "ax.set_ylim(5.5, -0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resposta (a)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "#calculos de entropia para verificaçã da materia ia660\n",
    "\n",
    "#(3/4)*(math.log(0.75,2))\n",
    "#math.log(3,2)\n",
    "#1/3\n",
    "#-(0.33*(math.log(0.33,2)))+(0.33*(math.log(0.33,2)))+(0.33*(math.log(0.33,2)))\n",
    "#math.log(3,2)\n",
    "hx = -np.log2(0.333)\n",
    "\n",
    "hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
